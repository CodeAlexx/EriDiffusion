//! Weight loading utilities for Flux models

use anyhow::Result;
use candle_core::{Device, DType, Tensor};
use std::path::Path;
use std::collections::HashMap;
use crate::loaders::lazy_safetensors::LazySafetensorsLoader;
use std::sync::Arc;

/// Load weights into a Flux model from a checkpoint
pub fn load_flux_weights_into_model(
    model: &mut super::FluxModelWithLoRA,
    checkpoint_path: &Path,
    device: &Device,
    dtype: DType,
) -> Result<()> {
    println!("Loading Flux weights from: {:?}", checkpoint_path);
    
    // Create lazy loader
    let loader = Arc::new(LazySafetensorsLoader::new(checkpoint_path, device.clone(), dtype)?);
    
    // Load weights in batches to avoid memory issues
    
    // 1. Load input projections
    println!("Loading input projections...");
    load_tensors_into_module(&loader, &[
        "img_in.weight",
        "img_in.bias",
        "txt_in.weight", 
        "txt_in.bias",
    ])?;
    
    // 2. Load time/vector embeddings
    println!("Loading embeddings...");
    load_tensors_into_module(&loader, &[
        "time_in.in_layer.weight",
        "time_in.in_layer.bias",
        "time_in.out_layer.weight",
        "time_in.out_layer.bias",
        "vector_in.in_layer.weight",
        "vector_in.in_layer.bias",
        "vector_in.out_layer.weight",
        "vector_in.out_layer.bias",
    ])?;
    
    // 3. Load double blocks (one at a time to save memory)
    for i in 0..19 {
        println!("Loading double block {}...", i);
        let prefix = format!("double_blocks.{}", i);
        load_double_block_weights(&loader, &prefix)?;
    }
    
    // 4. Load single blocks
    for i in 0..38 {
        if i % 5 == 0 {
            println!("Loading single blocks {}-{}...", i, (i+5).min(38));
        }
        let prefix = format!("single_blocks.{}", i);
        load_single_block_weights(&loader, &prefix)?;
    }
    
    // 5. Load final layer
    println!("Loading final layer...");
    load_tensors_into_module(&loader, &[
        "final_layer.linear.weight",
        "final_layer.linear.bias",
    ])?;
    
    println!("âœ… Weights loaded successfully!");
    Ok(())
}

/// Load tensors for a specific module
fn load_tensors_into_module(
    loader: &Arc<LazySafetensorsLoader>,
    tensor_names: &[&str],
) -> Result<()> {
    for name in tensor_names {
        if loader.contains(name) {
            let _tensor = loader.load_tensor(name)?;
            // TODO: Actually apply the tensor to the model
            // For now, just loading to verify it works
        }
    }
    Ok(())
}

/// Load weights for a double block
fn load_double_block_weights(
    loader: &Arc<LazySafetensorsLoader>,
    prefix: &str,
) -> Result<()> {
    // Modulation weights
    for mod_type in ["img_mod", "txt_mod"] {
        let mod_name = format!("{}.{}.lin.weight", prefix, mod_type);
        if loader.contains(&mod_name) {
            let _w = loader.load_tensor(&mod_name)?;
        }
        let bias_name = format!("{}.{}.lin.bias", prefix, mod_type);
        if loader.contains(&bias_name) {
            let _b = loader.load_tensor(&bias_name)?;
        }
    }
    
    // Attention weights
    for attn_type in ["img_attn", "txt_attn"] {
        // Try QKV combined first
        let qkv_weight = format!("{}.{}.qkv.weight", prefix, attn_type);
        if loader.contains(&qkv_weight) {
            let _qkv = loader.load_tensor(&qkv_weight)?;
            // TODO: Split and apply to q, k, v
        } else {
            // Load separate q, k, v
            for qkv in ["to_q", "to_k", "to_v"] {
                let w = format!("{}.{}.{}.weight", prefix, attn_type, qkv);
                let b = format!("{}.{}.{}.bias", prefix, attn_type, qkv);
                if loader.contains(&w) {
                    let _weight = loader.load_tensor(&w)?;
                }
                if loader.contains(&b) {
                    let _bias = loader.load_tensor(&b)?;
                }
            }
        }
        
        // Output projection
        let out_w = format!("{}.{}.proj.weight", prefix, attn_type);
        let out_b = format!("{}.{}.proj.bias", prefix, attn_type);
        if loader.contains(&out_w) {
            let _w = loader.load_tensor(&out_w)?;
        } else {
            // Try alternative naming
            let alt_w = format!("{}.{}.to_out.0.weight", prefix, attn_type);
            if loader.contains(&alt_w) {
                let _w = loader.load_tensor(&alt_w)?;
            }
        }
        if loader.contains(&out_b) {
            let _b = loader.load_tensor(&out_b)?;
        }
    }
    
    // MLP weights
    for mlp_type in ["img_mlp", "txt_mlp"] {
        let fc1_w = format!("{}.{}.0.weight", prefix, mlp_type);
        let fc1_b = format!("{}.{}.0.bias", prefix, mlp_type);
        let fc2_w = format!("{}.{}.2.weight", prefix, mlp_type);
        let fc2_b = format!("{}.{}.2.bias", prefix, mlp_type);
        
        for name in [fc1_w, fc1_b, fc2_w, fc2_b] {
            if loader.contains(&name) {
                let _t = loader.load_tensor(&name)?;
            }
        }
    }
    
    Ok(())
}

/// Load weights for a single block
fn load_single_block_weights(
    loader: &Arc<LazySafetensorsLoader>,
    prefix: &str,
) -> Result<()> {
    // Similar structure but simpler than double blocks
    
    // Modulation
    let mod_w = format!("{}.modulation.lin.weight", prefix);
    let mod_b = format!("{}.modulation.lin.bias", prefix);
    if loader.contains(&mod_w) {
        let _w = loader.load_tensor(&mod_w)?;
    }
    if loader.contains(&mod_b) {
        let _b = loader.load_tensor(&mod_b)?;
    }
    
    // Attention (similar to double blocks)
    let qkv_weight = format!("{}.attn.qkv.weight", prefix);
    if loader.contains(&qkv_weight) {
        let _qkv = loader.load_tensor(&qkv_weight)?;
    }
    
    // MLP
    let mlp_names = [
        format!("{}.mlp.0.weight", prefix),
        format!("{}.mlp.0.bias", prefix),
        format!("{}.mlp.2.weight", prefix),
        format!("{}.mlp.2.bias", prefix),
    ];
    
    for name in mlp_names {
        if loader.contains(&name) {
            let _t = loader.load_tensor(&name)?;
        }
    }
    
    Ok(())
}